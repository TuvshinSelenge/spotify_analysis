{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling (Responsible: A)\n",
    "\n",
    "a. Identify suitable data mining algorithms and select one of these as the most suitable for your\n",
    "experiments, providing a justification for the selection.\n",
    "\n",
    "b. Identify the hyper-parameters available for tuning in your chosen model and select one that\n",
    "you deem most relevant for tuning, providing a justification.\n",
    "\n",
    "c. Define and document a train / validation / test set split, considering where necessary\n",
    "appropriate stratification, any dependencies between data instances (e.g. time series data)\n",
    "and relative sizes of the respective subsets.\n",
    "\n",
    "d. Train the model on the training set and compare the performance on the validation set to\n",
    "identify the best hyper-parameter setting, explicitly documenting all parameter settings\n",
    "tested (avoid stating simply to have used “default parameters”, focus on reproducibility of the\n",
    "results you report). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a\n",
    "\n",
    "For our data mining tast we have choosen the following algorithms:\n",
    "\n",
    "- Ordinary Least Squares (OLS)\n",
    "- Stochastic Gradient Descent (SDG)\n",
    "- Random Forest Regressor\n",
    "\n",
    "All these three models are easy to simple to understand and to implement and for large data samples as our good to handle in a reasonable time.\n",
    "\n",
    "Least Squares: Its a linear model with coefficients to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "\n",
    "Stochastic Gradient Descent: The gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both.\n",
    "\n",
    "Random Forest: A random forest is a meta estimator that fits a number of decision tree regressors on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_name</th>\n",
       "      <th>track_artist</th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>track_album_id</th>\n",
       "      <th>track_album_name</th>\n",
       "      <th>track_album_release_date</th>\n",
       "      <th>playlist_name</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>playlist_genre</th>\n",
       "      <th>...</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6f807x0ima9a1j3VPbc7VN</td>\n",
       "      <td>I Don't Care (with Justin Bieber) - Loud Luxury Remix</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>66</td>\n",
       "      <td>2oCs0DGTsRO98Gh5ZSl2Cx</td>\n",
       "      <td>I Don't Care (with Justin Bieber) [Loud Luxury Remix]</td>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>Pop Remix</td>\n",
       "      <td>37i9dQZF1DXcZDD7cfEKhW</td>\n",
       "      <td>pop</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.634</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.518</td>\n",
       "      <td>122.036</td>\n",
       "      <td>194754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0r7CVbZTWZgbTCYdfa2P31</td>\n",
       "      <td>Memories - Dillon Francis Remix</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>67</td>\n",
       "      <td>63rPSO264uRjW1X5E6cWv6</td>\n",
       "      <td>Memories (Dillon Francis Remix)</td>\n",
       "      <td>2019-12-13</td>\n",
       "      <td>Pop Remix</td>\n",
       "      <td>37i9dQZF1DXcZDD7cfEKhW</td>\n",
       "      <td>pop</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>-4.969</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0373</td>\n",
       "      <td>0.0724</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>0.3570</td>\n",
       "      <td>0.693</td>\n",
       "      <td>99.972</td>\n",
       "      <td>162600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1z1Hg7Vb0AhHDiEmnDE79l</td>\n",
       "      <td>All the Time - Don Diablo Remix</td>\n",
       "      <td>Zara Larsson</td>\n",
       "      <td>70</td>\n",
       "      <td>1HoSmj2eLcsrR0vE9gThr4</td>\n",
       "      <td>All the Time (Don Diablo Remix)</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>Pop Remix</td>\n",
       "      <td>37i9dQZF1DXcZDD7cfEKhW</td>\n",
       "      <td>pop</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.432</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0794</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.613</td>\n",
       "      <td>124.008</td>\n",
       "      <td>176616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75FpbthrwQmzHlBJLuGdC7</td>\n",
       "      <td>Call You Mine - Keanu Silva Remix</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>60</td>\n",
       "      <td>1nqYsOef1yKKuGOVchbsk6</td>\n",
       "      <td>Call You Mine - The Remixes</td>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>Pop Remix</td>\n",
       "      <td>37i9dQZF1DXcZDD7cfEKhW</td>\n",
       "      <td>pop</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.778</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>0.277</td>\n",
       "      <td>121.956</td>\n",
       "      <td>169093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1e8PAfcKUYoKkxPhrHqw4x</td>\n",
       "      <td>Someone You Loved - Future Humans Remix</td>\n",
       "      <td>Lewis Capaldi</td>\n",
       "      <td>69</td>\n",
       "      <td>7m7vv9wlQ4i0LFuJiE2zsQ</td>\n",
       "      <td>Someone You Loved (Future Humans Remix)</td>\n",
       "      <td>2019-03-05</td>\n",
       "      <td>Pop Remix</td>\n",
       "      <td>37i9dQZF1DXcZDD7cfEKhW</td>\n",
       "      <td>pop</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.672</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.725</td>\n",
       "      <td>123.976</td>\n",
       "      <td>189052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id  \\\n",
       "0  6f807x0ima9a1j3VPbc7VN   \n",
       "1  0r7CVbZTWZgbTCYdfa2P31   \n",
       "2  1z1Hg7Vb0AhHDiEmnDE79l   \n",
       "3  75FpbthrwQmzHlBJLuGdC7   \n",
       "4  1e8PAfcKUYoKkxPhrHqw4x   \n",
       "\n",
       "                                              track_name      track_artist  \\\n",
       "0  I Don't Care (with Justin Bieber) - Loud Luxury Remix        Ed Sheeran   \n",
       "1                        Memories - Dillon Francis Remix          Maroon 5   \n",
       "2                        All the Time - Don Diablo Remix      Zara Larsson   \n",
       "3                      Call You Mine - Keanu Silva Remix  The Chainsmokers   \n",
       "4                Someone You Loved - Future Humans Remix     Lewis Capaldi   \n",
       "\n",
       "   track_popularity          track_album_id  \\\n",
       "0                66  2oCs0DGTsRO98Gh5ZSl2Cx   \n",
       "1                67  63rPSO264uRjW1X5E6cWv6   \n",
       "2                70  1HoSmj2eLcsrR0vE9gThr4   \n",
       "3                60  1nqYsOef1yKKuGOVchbsk6   \n",
       "4                69  7m7vv9wlQ4i0LFuJiE2zsQ   \n",
       "\n",
       "                                        track_album_name  \\\n",
       "0  I Don't Care (with Justin Bieber) [Loud Luxury Remix]   \n",
       "1                        Memories (Dillon Francis Remix)   \n",
       "2                        All the Time (Don Diablo Remix)   \n",
       "3                            Call You Mine - The Remixes   \n",
       "4                Someone You Loved (Future Humans Remix)   \n",
       "\n",
       "  track_album_release_date playlist_name             playlist_id  \\\n",
       "0               2019-06-14     Pop Remix  37i9dQZF1DXcZDD7cfEKhW   \n",
       "1               2019-12-13     Pop Remix  37i9dQZF1DXcZDD7cfEKhW   \n",
       "2               2019-07-05     Pop Remix  37i9dQZF1DXcZDD7cfEKhW   \n",
       "3               2019-07-19     Pop Remix  37i9dQZF1DXcZDD7cfEKhW   \n",
       "4               2019-03-05     Pop Remix  37i9dQZF1DXcZDD7cfEKhW   \n",
       "\n",
       "  playlist_genre  ... key  loudness  mode  speechiness  acousticness  \\\n",
       "0            pop  ...   6    -2.634     1       0.0583        0.1020   \n",
       "1            pop  ...  11    -4.969     1       0.0373        0.0724   \n",
       "2            pop  ...   1    -3.432     0       0.0742        0.0794   \n",
       "3            pop  ...   7    -3.778     1       0.1020        0.0287   \n",
       "4            pop  ...   1    -4.672     1       0.0359        0.0803   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  duration_ms  \n",
       "0          0.000000    0.0653    0.518  122.036       194754  \n",
       "1          0.004210    0.3570    0.693   99.972       162600  \n",
       "2          0.000023    0.1100    0.613  124.008       176616  \n",
       "3          0.000009    0.2040    0.277  121.956       169093  \n",
       "4          0.000000    0.0833    0.725  123.976       189052  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "#others\n",
    "import random\n",
    "import warnings\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, width=80, compact=True)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('spotify_songs.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.b\n",
    "\n",
    "For our hyperparameter tuning, we opted for a computationally less intensive approach by using RandomizedSearchCV, which provides faster results. Previously, we had implemented GridSearchCV, but this method was too resource-intensive and failed to produce results within a reasonable runtime.\n",
    "\n",
    "RandomizedSearchCV implements the fit and score methods. Additionally, it provides methods such as score_samples, predict, predict_proba, decision_function, transform, and inverse_transform, depending on the capabilities of the estimator used.\n",
    "\n",
    "The parameters of the estimator are optimized through a cross-validated search over specified parameter distributions. Unlike GridSearchCV, which exhaustively evaluates all possible parameter combinations, RandomizedSearchCV samples a fixed number of parameter settings from the specified distributions. The number of settings to evaluate is determined by the n_iter parameter.\n",
    "\n",
    "The best hyperparameters identified through this process, along with their corresponding values, will be stored and presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grid_rf = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "hyperparameter_grid_sgd = {\n",
    "    'loss': ['squared_loss', 'huber'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant', 'optimal'],\n",
    "    'eta0': [0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For faster results, we use a subsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'track_id', 'track_name', 'track_artist',\n",
    "    'track_album_id', 'track_album_name',\n",
    "    'playlist_id', 'playlist_name'\n",
    "]\n",
    "scale_features = ['duration_ms', 'tempo']\n",
    "categorical_features = ['playlist_genre', 'playlist_subgenre']\n",
    "\n",
    "def preprocess_features(X):\n",
    "    # Convert release date to datetime and extract the year\n",
    "    X = X.copy()\n",
    "    X['track_album_release_date'] = pd.to_datetime(\n",
    "        X['track_album_release_date'], format='ISO8601', errors='coerce'\n",
    "    )\n",
    "    X['album_release_year'] = X['track_album_release_date'].dt.year\n",
    "    X = X.drop(columns=['track_album_release_date'])\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    X = X.drop(columns=columns_to_drop)\n",
    "    X = X.dropna()\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.c \n",
    "\n",
    "### Train, Test and Validation Split\n",
    "\n",
    "The dataset is split in two steps: first into a train-validation set and a test set, and then further into separate train and validation sets. Before splitting the data, the target variable is removed to isolate the features for preprocessing.\n",
    "\n",
    "During preprocessing, several steps are performed to prepare the data for modeling:\n",
    "\t1.\tHandling Missing Values: Rows containing missing data are removed to ensure data consistency.\n",
    "\t2.\tDate Conversion and Feature Extraction: The release date of tracks is converted into a datetime format, and the release year is extracted as a new feature. This allows the model to utilize temporal information effectively.\n",
    "\t3.\tDropping Irrelevant Columns: Columns that are not relevant for model training, such as IDs, track names, and playlist names, are removed to avoid unnecessary noise in the dataset.\n",
    "\n",
    "These preprocessing steps are applied separately to the train, validation, and test sets to maintain consistency while preventing any information from one set leaking into another, therefore preventing data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(\"track_popularity\", axis=1)\n",
    "y = df[\"track_popularity\"]\n",
    "\n",
    "# Split data into train+validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split train+validation into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "\n",
    "X_train = preprocess_features(X_train)\n",
    "X_val = preprocess_features(X_val)\n",
    "X_test = preprocess_features(X_test)\n",
    "\n",
    "y_train = y_train.dropna()\n",
    "y_val = y_val.dropna()\n",
    "y_test = y_test.dropna()\n",
    "\n",
    "#column transformer\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', StandardScaler(), scale_features),\n",
    "        ('encode', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train_transformed = column_transformer.fit_transform(X_train)\n",
    "\n",
    "X_val_transformed = column_transformer.transform(X_val)\n",
    "X_test_transformed = column_transformer.transform(X_test)\n",
    "\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=hyperparameter_grid_rf,\n",
    "    n_iter=2, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit on the training data\n",
    "random_search_rf.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Best model evaluation on validation set\n",
    "best_rf = random_search_rf.best_estimator_\n",
    "val_predictions_rf = best_rf.predict(X_val_transformed)\n",
    "mse_rf = mean_squared_error(y_val, val_predictions_rf)\n",
    "mae_rf = mean_absolute_error(y_val, val_predictions_rf)\n",
    "r2_rf = r2_score(y_val, val_predictions_rf)\n",
    "\n",
    "# Format best parameters\n",
    "best_params_rf = pp.pformat(random_search_rf.best_params_)\n",
    "\n",
    "# Append results to the list\n",
    "results_list.append({\n",
    "    \"Model\": \"Random Forest Regressor\",\n",
    "    \"MSE\": mse_rf,\n",
    "    \"MAE\": mae_rf,\n",
    "    \"R2 Score\": r2_rf,\n",
    "    \"Best Parameters\": best_params_rf\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "# Fit on the training data\n",
    "lr.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_predictions_lr = lr.predict(X_val_transformed)\n",
    "mse_lr = mean_squared_error(y_val, val_predictions_lr)\n",
    "mae_lr = mean_absolute_error(y_val, val_predictions_lr)\n",
    "r2_lr = r2_score(y_val, val_predictions_lr)\n",
    "\n",
    "# Append results to the list\n",
    "results_list.append({\n",
    "    \"Model\": \"Linear Regression\",\n",
    "    \"MSE\": mse_lr,\n",
    "    \"MAE\": mae_lr,\n",
    "    \"R2 Score\": r2_lr,\n",
    "    \"Best Parameters\": \"N/A\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
      "9 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDRegressor must be a str among {'squared_epsilon_insensitive', 'epsilon_insensitive', 'huber', 'squared_error'}. Got 'squared_loss' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDRegressor must be a str among {'epsilon_insensitive', 'squared_error', 'squared_epsilon_insensitive', 'huber'}. Got 'squared_loss' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDRegressor must be a str among {'huber', 'squared_epsilon_insensitive', 'epsilon_insensitive', 'squared_error'}. Got 'squared_loss' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/tuvshinselenge/Documents/TU/BI/spotify_analysis/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [-247549.24478266   -3200.12768169              nan              nan\n",
      "              nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDRegressor(random_state=42, max_iter=1000)\n",
    "\n",
    "random_search_sgd = RandomizedSearchCV(\n",
    "    estimator=sgd,\n",
    "    param_distributions=hyperparameter_grid_sgd,\n",
    "    n_iter=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search_sgd.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Best model evaluation on validation set\n",
    "best_sgd = random_search_sgd.best_estimator_\n",
    "val_predictions_sgd = best_sgd.predict(X_val_transformed)\n",
    "mse_sgd = mean_squared_error(y_val, val_predictions_sgd)\n",
    "mae_sgd = mean_absolute_error(y_val, val_predictions_sgd)\n",
    "r2_sgd = r2_score(y_val, val_predictions_sgd)\n",
    "\n",
    "# Format best parameters\n",
    "best_params_sgd = pp.pformat(random_search_sgd.best_params_)\n",
    "\n",
    "results_list.append({\n",
    "    \"Model\": \"SGD Regressor\",\n",
    "    \"MSE\": mse_sgd,\n",
    "    \"MAE\": mae_sgd,\n",
    "    \"R2 Score\": r2_sgd,\n",
    "    \"Best Parameters\": best_params_sgd\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.d Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "                     Model          MSE        MAE  R2 Score  \\\n",
      "0  Random Forest Regressor   423.761535  16.630657   0.31979   \n",
      "1        Linear Regression   522.373969  18.866630   0.16150   \n",
      "2            SGD Regressor  2432.392549  42.537120  -2.90441   \n",
      "\n",
      "                                                                                                           Best Parameters  \n",
      "0  { 'bootstrap': True,\\n  'max_depth': None,\\n  'min_samples_leaf': 1,\\n  'min_samples_split': 2,\\n  'n_estimators': 100}  \n",
      "1                                                                                                                      N/A  \n",
      "2                { 'alpha': 0.001,\\n  'eta0': 0.1,\\n  'learning_rate': 'constant',\\n  'loss': 'huber',\\n  'penalty': 'l1'}  \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "results_df = results_df[[\n",
    "    \"Model\", \"MSE\", \"MAE\", \"R2 Score\", \"Best Parameters\"\n",
    "]]\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
